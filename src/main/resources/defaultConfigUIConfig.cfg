#Initial Maze
horizontal = true
initialPathLength = 5
numberOfWayColors = 5
numberOfWallColors = 5

#Maze Change
# These can be combined: CriterionA, CriterionB, ...
# MaxEpisodesReached(numberOfEpisodes = <int>)
# PerformanceAchievedPercentageTolerance(numberOfConsideredEpisodes = <int>, percentageTolerance = <double>)
# PerformanceAchievedStaticTolerance(numberOfConsideredEpisodes = <int>, numberOfToleranceActions = <int>)
levelChangeCriteria = MaxEpisodesReached(numberOfEpisodes = 20)
numberOfLevels = 10
delta = 200.0

#Reinforcement Learning
trainingName = GreedyHorizontal
qLearningAlpha = 1.0
qLearningGamma = 0.1
# DecreasingEpsilonPolicy(epsilon_0 = <double>, reducingFactor = <double>, seed = <int>)
# EpsilonFirstPolicy(epsilonExplore = <double>, epsilonExploit = <double>, numberOfExploringActions = <int>, seed = <int>)
# EpsilonGreedyPolicy(epsilon = <double>, seed = <int>)
# GreedyPolicy(seed = <int>)
# RandomPolicy(seed = <int>)
# SoftmaxPolicy(temperature = <double>, precision = <int>, seed = <int>)
# VDBEPolicy(inverseSensitivity = <double>, epsilon_0 = <double>, seed = <int>)
explorationPolicy = GreedyPolicy(seed = 1456)
# These can be combined: CriterionA, CriterionB, ...
# AgentExceedsOptimalPathPercentage(percentageOfExtraActions = <double>)
# AgentExceedsOptimalPathStatic(numberOfExtraActions = <int>)
# EndStateReached()
# MaxActionsReached(maxActions = <int>)
episodeStoppingCriteria = EndStateReached(), MaxActionsReached(maxActions = 100)


##########################################################################
# DO NOT CHANGE THE FOLLOWING PART IF YOU DO NOT KNOW WHAT YOU ARE DOING #
##########################################################################


#Initial Maze
generatedWayColorsSeed = 8913
generatedWallColorsSeed = 9122
usedWayColorsSeed = 123
usedWallColorsSeed = 345
minWallWayBrightnessDifference = 200

#Maze Change
# DefaultComplexityFunction()
complexityFunction = DefaultComplexityFunction()
changeMazeSeed = 123
# These can be combined: OperatorA, OperatorB, ...
# ChangeOptimalPathOperator(costsPerOptimalPathLengthIncreasement = <double>, seed = <int>)
# DeadEndOperator(minPathLen = <int>, maxPathLen = <int>, costPerNode = <double>, optimalPathAndParallelRoutesPreferencePercentage = <double>, seed = <int>)
# NewPathOperator(minPathLen = <int>, maxPathLen = <int>, costPerNode = <double>, seed = <int>)
# ResizeOperator(costsPerDimension = <double>, seed = <int>)
mazeOperators = ChangeOptimalPathOperator(costsPerOptimalPathLengthIncreasement = 20.0, seed = 12345), DeadEndOperator(minPathLen = 3, maxPathLen = 6, costPerNode = 40.0, optimalPathAndParallelRoutesPreferencePercentage = 0.95, seed = 123), ResizeOperator(costsPerDimension = 100.0,seed = 123), NewPathOperator(minPathLen = 4,maxPathLen = 10,costPerNode = 40.0,seed = 123)

#Reinforcement Learning
initialQValue = 0.0
wayNodeReward = -0.1
endNodeReward = 1.0
startEachLevelWithEmptyQTable = false

#Features
restrictImageSize = true
showProgressBarInConsole = true
