#Initial Maze
horizontal = true
initialPathLength = 5
numberOfWayColors = 10
numberOfWallColors = 10

#Maze Change
# These can be combined: CriterionA, CriterionB, ...
# MaxEpisodesReached(numberOfEpisodes = <int>)
# PerformanceAchievedPercentageTolerance(numberOfConsideredEpisodes = <int>, percentageTolerance = <double>)
# PerformanceAchievedStaticTolerance(numberOfConsideredEpisodes = <int>, numberOfToleranceActions = <int>)
levelChangeCriteria = MaxEpisodesReached(numberOfEpisodes = 10)
numberOfLevels = 10
delta = 200.0

#Reinforcement Learning
qLearningAlpha = 0.1
qLearningGamma = 1.0
# DecreasingEpsilonPolicy(epsilon_0 = <double>, reducingFactor = <double>, seed = <int>)
# EpsilonFirstPolicy(epsilonExplore = <double>, epsilonExploit = <double>, numberOfExploringActions = <int>, seed = <int>)
# EpsilonGreedyPolicy(epsilon = <double>, seed = <int>)
# GreedyPolicy(seed = <int>)
# RandomPolicy(seed = <int>)
# SoftmaxPolicy(temperature = <double>, precision = <int>, seed = <int>)
# VDBEPolicy(inverseSensitivity = <double>, epsilon_0 = <double>, seed = <int>)
explorationPolicy = EpsilonGreedyPolicy(epsilon = 0.1, seed = 123)
# These can be combined: CriterionA, CriterionB, ...
# AgentExceedsOptimalPathPercentage(percentageOfExtraActions = <double>)
# AgentExceedsOptimalPathStatic(numberOfExtraActions = <int>)
# EndStateReached()
# MaxActionsReached(maxActions = <int>)
episodeStoppingCriteria = EndStateReached()

#Misc
trainingName = demoConfig

##########################################################################
# DO NOT CHANGE THE FOLLOWING PART IF YOU DO NOT KNOW WHAT YOU ARE DOING #
##########################################################################


#Initial Maze
generatedWayColorsSeed = 8913
generatedWallColorsSeed = 9122
usedWayColorsSeed = 123
usedWallColorsSeed = 345
minWallWayBrightnessDifference = 200

#Maze Change
# DefaultComplexityFunction()
complexityFunction = DefaultComplexityFunction()
changeMazeSeed = 999
# These can be combined: OperatorA, OperatorB, ...
# ChangeOptimalPathOperator(costsPerOptimalPathLengthIncreasement = <double>, seed = <int>)
# DeadEndOperator(minPathLen = <int>, maxPathLen = <int>, costPerNode = <double>, optimalPathAndParallelRoutesPreferencePercentage = <double>, seed = <int>)
# NewPathOperator(minPathLen = <int>, maxPathLen = <int>, costPerNode = <double>, seed = <int>)
# ResizeOperator(costsPerDimension = <double>, seed = <int>)
mazeOperators = ChangeOptimalPathOperator(costsPerOptimalPathLengthIncreasement = 5.0, seed = 123), DeadEndOperator(minPathLen = 3, maxPathLen = 50, costPerNode = 40.0, optimalPathAndParallelRoutesPreferencePercentage = 0.5, seed = 123), ResizeOperator(costsPerDimension = 25.0,seed = 123), NewPathOperator(minPathLen = 4,maxPathLen = 50,costPerNode = 10.0,seed = 123)

#Reinforcement Learning
initialQValue = 0.0
wayNodeReward = -0.1
endNodeReward = 1.0
startEachLevelWithEmptyQTable = false

#Misc
restrictImageSize = true
showProgressBarInConsole = true
